# 6.4 Remote State Storage

While local state files work for development, production environments require remote state storage. Remote backends provide the security, availability, and collaboration features essential for team-based infrastructure management.

## Why Remote State?

**Local State Limitations**:
- Single point of failure (hardware crashes)
- No team collaboration
- Security risks (unencrypted files)
- No concurrent access protection

**Remote State Benefits**:
- Centralized storage with backups
- Team access with proper permissions
- State locking prevents conflicts
- Encryption at rest and in transit
- Version history and rollback capabilities

## Popular Backend Options

Choose a backend based on your existing infrastructure and team preferences:

### Cloud Provider Backends

**AWS S3 Backend**:
```hcl
terraform {
  backend "s3" {
    bucket         = "my-terraform-state"
    key            = "prod/terraform.tfstate"
    region         = "us-west-2"
    dynamodb_table = "terraform-locks"
    encrypt        = true
  }
}
```

**Azure Storage Backend**:
```hcl
terraform {
  backend "azurerm" {
    resource_group_name  = "terraform-state-rg"
    storage_account_name = "terraformstate"
    container_name       = "tfstate"
    key                  = "prod.terraform.tfstate"
  }
}
```

**Google Cloud Storage Backend**:
```hcl
terraform {
  backend "gcs" {
    bucket = "my-terraform-state"
    prefix = "prod"
  }
}
```

### Terraform Cloud/Enterprise

**Cloud Backend**:
```hcl
terraform {
  cloud {
    organization = "my-org"
    workspaces {
      name = "my-workspace"
    }
  }
}
```

### Backend Selection Guidelines

| Use Case | Recommended Backend | Why |
|----------|-------------------|-----|
| AWS Infrastructure | S3 + DynamoDB | Native integration, cost-effective |
| Azure Infrastructure | AzureRM | Native integration with Azure |
| GCP Infrastructure | GCS | Native integration with Google Cloud |
| Multi-cloud | Terraform Cloud | Vendor-neutral, advanced features |
| Self-hosted | Consul or PostgreSQL | Full control, on-premises |

## Backend Configuration Best Practices

### Essential Security Requirements

1. **Access Control**: Restrict backend access to authorized users only
2. **Encryption**: Enable encryption at rest and in transit
3. **State Locking**: Prevent concurrent modifications
4. **Audit Logging**: Track all state access and modifications
5. **Backup Strategy**: Regular, tested backups with retention policies

### AWS S3 Backend Setup

**Step 1: Create S3 Bucket and DynamoDB Table**
```bash
# Create S3 bucket
aws s3 mb s3://my-terraform-state --region us-west-2

# Enable versioning
aws s3api put-bucket-versioning \
  --bucket my-terraform-state \
  --versioning-configuration Status=Enabled

# Create DynamoDB table for locking
aws dynamodb create-table \
  --table-name terraform-locks \
  --attribute-definitions AttributeName=LockID,AttributeType=S \
  --key-schema AttributeName=LockID,KeyType=HASH \
  --billing-mode PAY_PER_REQUEST \
  --region us-west-2
```

**Step 2: Configure Backend**
```hcl
terraform {
  backend "s3" {
    bucket         = "my-terraform-state"
    key            = "environments/prod/terraform.tfstate"
    region         = "us-west-2"
    dynamodb_table = "terraform-locks"
    encrypt        = true
  }
}
```


6.4.2 Configuring the Backend: Best Practices & Checklist

Every backend is different, but the following checklist applies to all production-grade remote state setups:

| Practice                | Why It Matters                                              |
|-------------------------|------------------------------------------------------------|
| Restrict access         | Only CI/CD or admins should access state, not all devs      |
| Enable access logging   | Audit who accessed/changed state for security & debugging   |
| Reliable backups        | Recover quickly from corruption or accidental deletion      |
| Encryption at rest      | Protect sensitive data in state (passwords, secrets, etc.)  |
| State locking           | Prevent concurrent changes and state corruption             |

**How to apply these:**
- Cloud backends (S3, GCS, AzureRM) often have built-in encryption and locking, but you may need to enable/configure them.
- For self-managed backends (Consul, etcd), read the backend’s docs and security guides—defaults are not always safe!

> **Pro Tip:** Test your backup and restore process regularly. A backup you haven’t tested is as good as no backup at all. Schedule disaster recovery drills so your team knows what to do when it matters.


6.4.3 The `backend` Block: How and Where to Use It

The backend block configures remote state and belongs **only in the root module** (never in child modules). Each backend has its own parameters—always check the docs for required and optional fields.

**Example: S3 Backend Block**
```hcl
terraform {
  backend "s3" {
    bucket         = "my-terraform-state"
    key            = "prod/terraform.tfstate"
    region         = "us-west-2"
    dynamodb_table = "terraform-locks"
    encrypt        = true
  }
}
```

**Common pitfalls:**
- Defining the backend block in a child module (it will be ignored)
- Forgetting to configure authentication (Terraform will prompt or fail)
- Not enabling locking (risking state corruption)

> **Tip:** For self-managed backends like Consul, you can use Docker Compose to test locally. Always secure tokens and sensitive config values!
 
volumes:
  consul-data:

① We want to pull the latest version of the Hashicorp Consul image.

② A volume allows us to upgrade the container without losing our state.

③ This token was created with the uuidgen shell utility.

In the same directory as that file, we need to run a command to launch our services. If you haven’t installed Docker yet, you should do so. The Docker website (https://docs.docker.com/engine/install/) has installation instructions for most systems. Once installed, run this command:

docker compose up -d
Now you will have a copy of Consul running locally that you can use for testing. Our next step is to configure our backend. To do this, we create a terraform block with a backend block inside of it. The backend block needs to be labeled consul to tell Terraform that’s the backend we want to use. Inside the backend block we need to put our Consul-specific configuration, which includes the host address for Consul and the access token we used in our docker compose file.

Listing 6.8:
terraform {                                                       ①
  backend "consul" {                                              ②
    address      = "localhost:8500"                               ③
    scheme       = "http"                                         ④
    path         = "path/to/save/state"
    access_token = "01a56e2d-a96a-4ca5-9d39-d5152015f533"         ⑤
  }
}

① This Terraform block should only be in the root-level module, as backends can only be defined once per project.

② The block label, in this case consul, tells Terraform what backend to use.

③ All of these options are Consul specific, and other backends use their own parameters.

④ This should normally be https for security reasons, but we’re running Consul locally.

⑤ Hardcoding credentials is a very bad practice. We’ll discuss how to avoid this in the next example.

You may have noticed that there are parameters for backends that don’t make sense to hardcode. It’s bad practice to commit credentials into your code repositories, and you often want your parameters to be flexible so your code can power multiple environments. Terraform supports that by allowing you to define partial configurations in your code, with the expectation that you’ll fill in the rest of the parameters using another method.

Since there are very few parameters that should actually be hardcoded, it’s extremely common for the backend block to be completely empty. Terraform still needs that block to exist so it knows what backend to use, so it can’t be excluded.

Listing 6.9:
## Authentication Methods

### Secure Authentication Options

**Environment Variables** (Recommended for CI/CD):
```bash
# AWS
export AWS_ACCESS_KEY_ID="your-access-key"
export AWS_SECRET_ACCESS_KEY="your-secret-key"

# Azure
export ARM_CLIENT_ID="your-client-id"
export ARM_CLIENT_SECRET="your-client-secret"
export ARM_TENANT_ID="your-tenant-id"
export ARM_SUBSCRIPTION_ID="your-subscription-id"

# GCP
export GOOGLE_CREDENTIALS="path/to/service-account.json"
```

**Cloud Provider CLI** (Recommended for local development):
```bash
# AWS CLI
aws configure

# Azure CLI
az login

# GCP CLI
gcloud auth application-default login
```

**Partial Backend Configuration**:
```hcl
# main.tf - No sensitive data
terraform {
  backend "s3" {
    encrypt = true
  }
}
```

```bash
# Initialize with config file
terraform init -backend-config=backend.conf

# backend.conf
bucket = "my-terraform-state"
key    = "prod/terraform.tfstate"
region = "us-west-2"
```

## Backend Migration

Terraform makes it easy to migrate between backends:

### Migration Process

1. **Update Backend Configuration**:
```hcl
# Change from local to S3
terraform {
  backend "s3" {
    bucket = "my-terraform-state"
    key    = "terraform.tfstate"
    region = "us-west-2"
  }
}
```

2. **Run Migration**:
```bash
# Terraform detects the change and prompts for migration
terraform init -migrate-state
```

3. **Verify Migration**:
```bash
# Confirm state is accessible
terraform plan
```

### Common Migration Scenarios

| From | To | Use Case |
|------|----|---------| 
| Local | S3 | Moving to production |
| S3 | Terraform Cloud | Enhanced collaboration |
| One S3 bucket | Another S3 bucket | Reorganizing infrastructure |

## Workspaces

Workspaces allow multiple state files with the same configuration:

### Workspace Commands

```bash
# List workspaces
terraform workspace list

# Create new workspace
terraform workspace new staging

# Switch workspace
terraform workspace select production

# Delete workspace
terraform workspace delete staging
```

### Using Workspaces in Configuration

```hcl
locals {
  environment_config = {
    production = {
      instance_count = 3
      instance_type  = "t3.large"
    }
    staging = {
      instance_count = 1
      instance_type  = "t3.small"
    }
  }
  
  current_config = local.environment_config[terraform.workspace]
}

resource "aws_instance" "app" {
  count         = local.current_config.instance_count
  instance_type = local.current_config.instance_type
  
  tags = {
    Environment = terraform.workspace
  }
}
```

### Workspace Best Practices

1. **Use descriptive names**: `production`, `staging`, `dev-john`
2. **Set defaults**: Handle the `default` workspace gracefully
3. **Environment-specific configs**: Use `terraform.workspace` variable
4. **Separate sensitive data**: Different backends per environment for security

## Key Takeaways

### Production Backend Checklist

✅ **Security**:
- Enable encryption at rest and in transit
- Use IAM roles/service principals for authentication
- Restrict access to state files
- Enable audit logging

✅ **Reliability**:
- Configure state locking (DynamoDB for S3)
- Enable versioning and backups
- Test backup restoration procedures
- Monitor backend health

✅ **Collaboration**:
- Use remote backends for team access
- Implement proper access controls
- Document backend configuration
- Plan migration strategies

### Common Pitfalls to Avoid

❌ **Never**:
- Hardcode credentials in configuration
- Use local backends for production
- Skip state locking configuration
- Ignore backup testing

✅ **Always**:
- Use environment variables or CLI authentication
- Test backend migrations in non-production first
- Monitor state file size and performance
- Keep backend configurations in version control
- Use secure authentication methods for all environments
- Regularly review and update backend configurations

⑤ We populate our current settings from our big list using the workspace variable.


---
**NOTE: Workspace Confusion & Backend Upgrades**

HashiCorp uses the term "workspaces" for two different concepts:
- **Classic workspaces:** Share code and resources, used for local and most remote backends.
- **Terraform Cloud workspaces:** Each is a fully independent environment (code, state, variables, runs).

When using the `cloud` backend, the `terraform workspace` command refers to the cloud version, not the classic one. If you want to use classic workspaces for local dev but Terraform Cloud for production, consider using the local backend for dev.

**Backend Upgrades:**
- Backends are upgraded with Terraform itself—most of the time, no action is needed.
- Sometimes, backend parameters change or are removed (e.g., v1.3 removed legacy backends without locking).
- **Always read the upgrade notes** when changing Terraform versions to avoid breaking your backend config.

> **Summary:** Know which workspace system you’re using, and always check upgrade notes before changing Terraform versions or backend settings!